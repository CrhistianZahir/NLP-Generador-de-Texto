{
  "cells": [
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "mX8gZlVyCCbz"
      },
      "source": [
        "# Modelos del lenguaje con RNNs\n",
        "\n"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "QI274F8LQC59"
      },
      "source": [
        "## 1. Carga y procesado del texto"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "D7tKOZ9BFfki"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import keras\n",
        "import matplotlib.pyplot as plt\n",
        "from keras.callbacks import LambdaCallback\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense\n",
        "from keras.layers import Dropout\n",
        "from keras.layers import LSTM\n",
        "import sys\n",
        "import random\n",
        "import io\n",
        "\n",
        "path = keras.utils.get_file(\n",
        "    fname=\"don_quijote.txt\",\n",
        "    origin=\"https://onedrive.live.com/download?cid=C506CF0A4F373B0F&resid=C506CF0A4F373B0F%219424&authkey=AH0gb-qSo5Xd7Io\"\n",
        ")"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "VYGLvjLXrUUd"
      },
      "source": [
        "\n",
        "**1.1.** Leer todo el contenido del texto en una única variable ***text*** y convertir el string a minúsculas"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "8WB6FejrrTu9"
      },
      "outputs": [],
      "source": [
        "with open(path, encoding=\"utf8\") as f:\n",
        "  text = f.read().lower()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "knyzgX0XnGfm",
        "outputId": "bb56d544-87a2-4389-f60d-635d6c53caad"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "2071198"
            ]
          },
          "execution_count": 3,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "len(text)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "dkgGl8GWtUk8"
      },
      "source": [
        "Puede comprobar que se ha realizado la variación y que el texto se encuentra en minuscula."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hMFhe3COFwSD",
        "outputId": "61cf0565-a07b-4423-9064-b92202059899"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Longitud del texto: 2071198\n",
            "capítulo primero. que trata de la condición y ejercicio del famoso hidalgo\n",
            "don quijote de la mancha\n",
            "\n",
            "\n",
            "en un lugar de la mancha, de cuyo nombre no quiero acordarme, no ha mucho\n",
            "tiempo que vivía un hidalgo de los de lanza en astillero, adarga antigua,\n",
            "rocín flaco y galgo corredor. una olla de algo más vaca que carnero,\n",
            "salpicón las más noches, duelos y quebrantos los sábados, lantejas los\n",
            "viernes, algún palomino de añadidura los domingos, consumían las tres\n",
            "partes de su hacienda. el resto della co\n"
          ]
        }
      ],
      "source": [
        "#Se muestran los primeros 500 carácteres del texto y obtenemos la longitud del texto utilizando \"len(text)\".\n",
        "print(\"Longitud del texto: {}\".format(len(text)))\n",
        "print(text[0:500])"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "bZ7TUXWiyvOj"
      },
      "source": [
        "## 2. Procesado de los datos"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "mkfJUIxW5m5C"
      },
      "source": [
        "#### 2.1. Obtención de los caracteres y mapas de caracteres\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "5bJ0NsbCbupF"
      },
      "outputs": [],
      "source": [
        "chars=sorted(list(set(text)))\n",
        "char_indices = dict((c,i) for i, c in enumerate(chars))\n",
        "indice_char = dict((i, c) for i, c in enumerate(chars))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hTMi-94I7HKi",
        "outputId": "1fae8913-ef88-4d83-b8c4-7ebb157ac355"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "61"
            ]
          },
          "execution_count": 6,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "#indice_char\n",
        "char_indices\n",
        "len(chars)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "Y_B4AWo0ElwA"
      },
      "source": [
        "#### 2.2. Obtención de secuencias de entrada y carácter a predecir\n",
        "\n",
        "Ahora, se obtiene las secuencias de entrada en formato texto y los correspondientes caracteres a predecir. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "NslxhnnDK6uA"
      },
      "outputs": [],
      "source": [
        "# Definia el tamaño de las secuencias. Puede dejar este valor por defecto.\n",
        "SEQ_LENGTH = 35\n",
        "step=3\n",
        "rawX = []\n",
        "rawy = []\n",
        "\n",
        "for i in range(0, len(text) - SEQ_LENGTH, step):\n",
        "    rawX.append(text[i: i+SEQ_LENGTH])\n",
        "    rawy.append(text[i+SEQ_LENGTH])"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "1Y3AmjYtHdLJ"
      },
      "source": [
        "Indicar el tamaño del training set que acabamos de generar."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WVWqKxFcbwTu",
        "outputId": "647ab535-f992-4153-87ef-5d04b875a3cd"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Tamaño del training set generado:  690388\n"
          ]
        }
      ],
      "source": [
        "n_sentences=len(rawX)\n",
        "print(\"Tamaño del training set generado: \",n_sentences)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "goGQkKcwpLRJ"
      },
      "source": [
        "Como el Quijote es muy largo y tiene muchas secuencias, se puede encontrar con problemas de memoria. Por ello, se elije un número máximo de ellas."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "97SXdkJX7HKw",
        "outputId": "28699fd4-3f15-4304-f7f2-4337ed07cda4"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "100000\n"
          ]
        }
      ],
      "source": [
        "MAX_SEQUENCES = 100000\n",
        "\n",
        "perm = np.random.permutation(len(rawX)) #Permutar aleatoriamente una secuencia, o devolver un rango permutado.\n",
        "rawX, rawy = np.array(rawX), np.array(rawy)\n",
        "rawX, rawy = rawX[perm], rawy[perm]\n",
        "rawX, rawy = list(rawX[:MAX_SEQUENCES]), list(rawy[:MAX_SEQUENCES])\n",
        "\n",
        "print(len(rawX))"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "4FzgtAbPIs6f"
      },
      "source": [
        "#### 2.3. Obtención de input X y output y para el modelo\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "lBwJi3_Dsb2x"
      },
      "outputs": [],
      "source": [
        "X = np.zeros((len(rawX), SEQ_LENGTH , len(chars)))\n",
        "y = np.zeros((len(rawX), len(chars)))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "8F7lUsqLs5T9"
      },
      "outputs": [],
      "source": [
        "for i, sentence in enumerate(rawX):\n",
        "    for t, char in enumerate(sentence):\n",
        "        X[i, t, char_indices[char]] = 1\n",
        "    y[i, char_indices[rawy[i]]] = 1"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "IxeUxz3HPm3l"
      },
      "source": [
        "## 3. Definición del modelo y entrenamiento\n",
        "\n",
        "Se Define el modelo que utiliza una **LSTM** con **128 unidades internas**. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MSw2j0btYWZs",
        "outputId": "5e56b47d-1a72-4623-d328-1c3613995412"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " lstm (LSTM)                 (None, 128)               97280     \n",
            "                                                                 \n",
            " dropout (Dropout)           (None, 128)               0         \n",
            "                                                                 \n",
            " dense (Dense)               (None, 61)                7869      \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 105149 (410.74 KB)\n",
            "Trainable params: 105149 (410.74 KB)\n",
            "Non-trainable params: 0 (0.00 Byte)\n",
            "_________________________________________________________________\n"
          ]
        }
      ],
      "source": [
        "model= Sequential()\n",
        "model.add(LSTM(128, input_shape=(SEQ_LENGTH, len(chars))))\n",
        "model.add(Dropout(0.5))\n",
        "model.add(Dense(len(chars), activation= 'softmax'))\n",
        "model.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "BYrEhi3mxQoY"
      },
      "outputs": [],
      "source": [
        "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1B9RW3-Qxcf9",
        "outputId": "05a974fa-691b-4360-9518-4f86a5be512a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/15\n",
            "3125/3125 [==============================] - 144s 45ms/step - loss: 2.4775 - accuracy: 0.2843\n",
            "Epoch 2/15\n",
            "3125/3125 [==============================] - 142s 46ms/step - loss: 2.1345 - accuracy: 0.3543\n",
            "Epoch 3/15\n",
            "3125/3125 [==============================] - 144s 46ms/step - loss: 2.0293 - accuracy: 0.3831\n",
            "Epoch 4/15\n",
            "3125/3125 [==============================] - 149s 48ms/step - loss: 1.9570 - accuracy: 0.4026\n",
            "Epoch 5/15\n",
            "3125/3125 [==============================] - 142s 45ms/step - loss: 1.9032 - accuracy: 0.4152\n",
            "Epoch 6/15\n",
            "3125/3125 [==============================] - 143s 46ms/step - loss: 1.8569 - accuracy: 0.4316\n",
            "Epoch 7/15\n",
            "3125/3125 [==============================] - 143s 46ms/step - loss: 1.8179 - accuracy: 0.4405\n",
            "Epoch 8/15\n",
            "3125/3125 [==============================] - 145s 46ms/step - loss: 1.7823 - accuracy: 0.4521\n",
            "Epoch 9/15\n",
            "3125/3125 [==============================] - 142s 45ms/step - loss: 1.7559 - accuracy: 0.4597\n",
            "Epoch 10/15\n",
            "3125/3125 [==============================] - 143s 46ms/step - loss: 1.7309 - accuracy: 0.4679\n",
            "Epoch 11/15\n",
            "3125/3125 [==============================] - 144s 46ms/step - loss: 1.7051 - accuracy: 0.4751\n",
            "Epoch 12/15\n",
            "3125/3125 [==============================] - 145s 46ms/step - loss: 1.6839 - accuracy: 0.4794\n",
            "Epoch 13/15\n",
            "3125/3125 [==============================] - 143s 46ms/step - loss: 1.6673 - accuracy: 0.4870\n",
            "Epoch 14/15\n",
            "3125/3125 [==============================] - 143s 46ms/step - loss: 1.6483 - accuracy: 0.4911\n",
            "Epoch 15/15\n",
            "3125/3125 [==============================] - 143s 46ms/step - loss: 1.6325 - accuracy: 0.4947\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "<keras.src.callbacks.History at 0x7a068add4df0>"
            ]
          },
          "execution_count": 14,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "model.fit(X, y, batch_size=32, epochs=15, verbose=1)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "3yUFHS4kHkyY"
      },
      "source": [
        "Para ver cómo evoluciona SU modelo del lenguaje,  genere texto según va entrenando. Para ello, se programa una función que, utilizando el modelo en su estado actual, genera texto, con la idea de ver cómo se va generando texto al entrenar cada epoch.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "LoGYpWOHd7Lr"
      },
      "outputs": [],
      "source": [
        "def sample(probs, temperature=1.0):\n",
        "    \"\"\"Nos da el índice del elemento a elegir según la distribución\n",
        "    de probabilidad dada por probs.\n",
        "\n",
        "    Args:\n",
        "      probs es la salida dada por una capa softmax:\n",
        "        probs = model.predict(x_to_predict)[0]\n",
        "\n",
        "      temperature es un parámetro que nos permite obtener mayor\n",
        "        \"diversidad\" a la hora de obtener resultados.\n",
        "\n",
        "        temperature = 1 nos da la distribución normal de softmax\n",
        "        0 < temperature < 1 hace que el sampling sea más conservador,\n",
        "          de modo que sampleamos cosas de las que estamos más seguros\n",
        "        temperature > 1 hace que los samplings sean más atrevidos,\n",
        "          eligiendo en más ocasiones clases con baja probabilidad.\n",
        "          Con esto, tenemos mayor diversidad pero se cometen más\n",
        "          errores.\n",
        "    \"\"\"\n",
        "    # Cast a float64 por motivos numéricos\n",
        "    probs = np.asarray(probs).astype('float64')\n",
        "\n",
        "    # logaritmo de probabilidades y aplicamos reducción\n",
        "    # por temperatura.\n",
        "    probs = np.log(probs) / temperature\n",
        "\n",
        "    # Volvemos a aplicar exponencial y normalizamos de nuevo\n",
        "    exp_probs = np.exp(probs)\n",
        "    probs = exp_probs / np.sum(exp_probs)\n",
        "\n",
        "    # Hacemos el sampling dadas las nuevas probabilidades\n",
        "    # de salida (ver doc. de np.random.multinomial)\n",
        "    samples = np.random.multinomial(1, probs, 1)\n",
        "    return np.argmax(samples)\n"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "3fejfZldd4ou"
      },
      "source": [
        "Utilizando la función anterior y el modelo entrenado, se añade un callback a al modelo para que, según vaya entrenando, se vean los valores que resultan de generar textos con distintas temperaturas al acabar cada epoch.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "xOEZvnBXkODd"
      },
      "outputs": [],
      "source": [
        "TEMPERATURES_TO_TRY = [0.2] #, 0.5, 1.0, 1.2]\n",
        "GENERATED_TEXT_LENGTH = 300\n",
        "\n",
        "def generate_text(seed_text, model, length=300, temperature=1, max_length=30):\n",
        "    \"\"\"Genera una secuencia de texto a partir de seed_text utilizando model.\n",
        "\n",
        "    La secuencia tiene longitud length y el sampling se hace con la temperature\n",
        "    definida.\n",
        "    \"\"\"\n",
        "\n",
        "    # Aquí guardaremos nuestro texto generado, que incluirá el\n",
        "    # texto origen\n",
        "    generated = seed_text\n",
        "\n",
        "    # Utilizar el modelo en un bucle de manera que generemos\n",
        "    # carácter a carácter. Habrá que construir los valores de\n",
        "    # X_pred de manera similar a como hemos hecho arriba, salvo que\n",
        "    # aquí sólo se necesita una oración\n",
        "    # Nótese que el x que utilicemos tiene que irse actualizando con\n",
        "    # los caracteres que se van generando. La secuencia de entrada al\n",
        "    # modelo tiene que ser una secuencia de tamaño SEQ_LENGTH que\n",
        "    # incluya el último caracter predicho.\n",
        "\n",
        "    prediction = []\n",
        "\n",
        "    for i in range(length):\n",
        "        # Make numpy array to hold seed\n",
        "        X = np.zeros((1, len(generated), len(chars) ))\n",
        "\n",
        "        # Set one-hot vectors for seed sequence\n",
        "        for t, char in enumerate(seed_text):\n",
        "            X[0, t, char_indices[char]] = 1\n",
        "\n",
        "        # Generate prediction for next character\n",
        "        preds = model.predict(X, verbose=0)[0]\n",
        "        # Choose a character from the prediction probabilities\n",
        "        next_index = sample(preds,0.2)\n",
        "        next_char = indice_char[next_index]\n",
        "\n",
        "        prediction.append(next_char)\n",
        "        # Add the predicted character to the seed sequence so the next prediction\n",
        "        # includes this character in it's seed.\n",
        "        #generated += next_char\n",
        "        seed_text = seed_text[1:] + next_char\n",
        "\n",
        "        print(next_char, end= \" \");\n",
        "        # Flush so we can see the prediction as it's generated\n",
        "        sys.stdout.flush()\n",
        "\n",
        "    prediction = ''.join(prediction)\n",
        "    sys.stdout.flush()\n",
        "\n",
        "    return generated\n",
        "\n",
        "\n",
        "def on_epoch_end(epoch, logs):\n",
        "  print(\"\\n\\n\\n\")\n",
        "\n",
        "  # Primero, seleccionamos una secuencia al azar para empezar a predecir\n",
        "  # a partir de ella\n",
        "  start_pos = random.randint(0, len(text) - SEQ_LENGTH - 1)\n",
        "  seed_text = text[start_pos:start_pos + SEQ_LENGTH]\n",
        "  for temperature in TEMPERATURES_TO_TRY:\n",
        "    print(\"------> Epoch: {} - Generando texto con temperature {}\".format(\n",
        "        epoch + 1, temperature))\n",
        "\n",
        "    generated_text = generate_text(seed_text, model,\n",
        "                                   GENERATED_TEXT_LENGTH, temperature)\n",
        "    print(\"Seed: {}\".format(seed_text))\n",
        "    print(\"Texto generado: {}\".format(generated_text))\n",
        "\n",
        "\n",
        "generation_callback = LambdaCallback(on_epoch_end=on_epoch_end)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3oT7pNvjrP2e",
        "outputId": "93b05605-5d53-4def-df14-c8a1e3dd559f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/15\n",
            "3125/3125 [==============================] - ETA: 0s - loss: 1.6052 - accuracy: 0.5042\n",
            "\n",
            "\n",
            "\n",
            "------> Epoch: 1 - Generando texto con temperature 0.2\n",
            "n   p e n s a d o   d e   l a   m e r a d o ,   s i n o   e s t a b a   d e   s u   p a r t e   d e   l a   c a b a l l e r o   d e   l a   m e r a d a   d e   l a   m a n o ,   y   a   s u   s e ñ o r   d e   l a   m e r a d a   d e   s u   c a b a l l e r o   a n d a n t e   s e   h a b í a   d e   d e l   c o m p e r o   d e   s u   s e ñ o r   d e   l a   m e r a d a ,   d e   s u   m e r a d o   d e   l a   m e r a d   d e   l a s   c a b a l l e r o s   d e   l a   d e s t a   a l l a   d e   l a   m e r a d o ,   y   d e   m u c h a   d e   l a   m a l a   d e   s u   p e r s e n t e   Seed: , y, aunque tenía más cuartos que u\n",
            "Texto generado: , y, aunque tenía más cuartos que u\n",
            "3125/3125 [==============================] - 170s 54ms/step - loss: 1.6052 - accuracy: 0.5042\n",
            "Epoch 2/15\n",
            "3124/3125 [============================>.] - ETA: 0s - loss: 1.5894 - accuracy: 0.5077\n",
            "\n",
            "\n",
            "\n",
            "------> Epoch: 2 - Generando texto con temperature 0.2\n",
            "e   s e   l a   c a b a l l e r o   q u e   e s t a b a   l a   m a n o   p o r   e s t a   d e   l a   c a b a l l e r o   d e   l a   c u a l   d e   l a   d e l   c u r a   y   e s t a b a   d e   l a   c a b a l l e r o   d e   l a   c a b a l l e   d e   l a   c a b a l l e   d e   s u   s e ñ o r   d e   l a   m a n o   e n   l a   c a b a l l e r o   d e   l a   m a n o   q u e   e s   s i   l o   q u e   l e   h a b í a   d e   s e r   d e   d e s t a n   d e   l a   c a r a   d e   l a   c a b a l l e r o   a n d a n t e   d e   l a   c a b a l l e ,   a   l a   c a b a l l e r o   d Seed: eñor don quijote de la mancha, y qu\n",
            "Texto generado: eñor don quijote de la mancha, y qu\n",
            "3125/3125 [==============================] - 170s 54ms/step - loss: 1.5895 - accuracy: 0.5077\n",
            "Epoch 3/15\n",
            "3125/3125 [==============================] - ETA: 0s - loss: 1.5735 - accuracy: 0.5128\n",
            "\n",
            "\n",
            "\n",
            "------> Epoch: 3 - Generando texto con temperature 0.2\n",
            "e r   d e   l a   m e n o s   d e   l a   m e s t a d a   d e   l a   m a n o ,   y   a   l a   c a b a l l e r o   a n d a n t e   s e   l e   c u a n d o   e n   l a   c a b a l l e r o   d e   l a   m e n o s   d e   l a   c o n t e n t a   d e   l a   c a b a l l e r o   d e   l a   c a b a l l e r o   q u e   p o n   a   l o s   a l l a s   d e   l a   m a n t e r   d e   l a   c u a l   d e   l a   m e n o s ,   y   a   l a   m e s t a d a   d e   l a   c a b a l l e r o   a n d a n t e   e n   l a   a l l a   d e   l a   p r e s t a   d e   l a   m e n o s   d e   s u   p o n e r   a   Seed: ue oyó conjurarse, por su temor col\n",
            "Texto generado: ue oyó conjurarse, por su temor col\n",
            "3125/3125 [==============================] - 170s 54ms/step - loss: 1.5735 - accuracy: 0.5128\n",
            "Epoch 4/15\n",
            "3124/3125 [============================>.] - ETA: 0s - loss: 1.5594 - accuracy: 0.5169\n",
            "\n",
            "\n",
            "\n",
            "------> Epoch: 4 - Generando texto con temperature 0.2\n",
            "  e s t a   a l g u n a   c o n s e g u i e r e   a   l o s   d e   l a   c a b a l l e r o   d e   l a   m a n t e   d e   l a   m e n t e r   d e   l a   c a s a   d e   l a   c a r a   d e   l a   m e r c e d   d e   l a   c a b a l l e r o   a n d a n t e   s e   h a b í a   d e   l a   m e n t e r   d e   l a   c a r a   d e   l a   m a n t e   d e   l a   m e n t e r   d e   l a   m a n c h a ,   y   a l   d e   l a   m e n t e r   d e   l a   c o n c e n a d o ,   y   d e   l a   m e n t e   d e   l a   m e n c i d a   d e   l a   c a r t e   d e   l a   v e n t a   d e   l a   m a n t Seed:  habían sido de sus\n",
            "bisabuelos, que\n",
            "Texto generado:  habían sido de sus\n",
            "bisabuelos, que\n",
            "3125/3125 [==============================] - 175s 56ms/step - loss: 1.5593 - accuracy: 0.5169\n",
            "Epoch 5/15\n",
            "3125/3125 [==============================] - ETA: 0s - loss: 1.5487 - accuracy: 0.5198\n",
            "\n",
            "\n",
            "\n",
            "------> Epoch: 5 - Generando texto con temperature 0.2\n",
            "  d e   l a   c a b a l l e r o   a n d a n t e   d e   l a   c a r a c i a   d e   l a   c a r a z a   q u e   l a   c a r t e   l a   c a b a l l e r o   d e   l a   m e n a d o   d e   l a   c a r t e ,   y   a l   s e   l e   d i j o   e l   c a r a l l e   c o n   l a   m e r a d o   e n   l a   c a b a l l e r o   a n d a n t e   s e   a l g u n a   d e   l a   m e n o s   d e   l a   d e l   a   l a   m e n a s   d e   l a   m e r a d o   e n   l a   m a n o   q u e   s e   l e   d i j o   e l   c a b a l l e r o   d e   l a   m e n a s   d e   l a   m e n a s   d e   l a   c a r a c i Seed: ispuesta. vosotros estáis ya en eda\n",
            "Texto generado: ispuesta. vosotros estáis ya en eda\n",
            "3125/3125 [==============================] - 172s 55ms/step - loss: 1.5487 - accuracy: 0.5198\n",
            "Epoch 6/15\n",
            "3124/3125 [============================>.] - ETA: 0s - loss: 1.5360 - accuracy: 0.5224\n",
            "\n",
            "\n",
            "\n",
            "------> Epoch: 6 - Generando texto con temperature 0.2\n",
            "c o s t a d o   d e   l a   m e s t e   h a b í a   d e   l a   c a s a   d e   l a   c a b a l l e r o   d e   l a   m a n o   e n   l a   m e s c e d   a l   c u r a   y   d e   m u c h a   s e ñ o r   d e   l a   c o n t e n t a   d e   l a   c a r t e   d e   l a   m e n c i a   d e   l a   m a n o   a   l o   q u e   l a   c o n t a d o ,   y   e s t a   a l g u n a   d e   l a   c a b a l l e r o   a n d a n t e   d e   l a   c u a l   d e   l a   c a s a   d e   l a   c a s a   d e   l a   m e n t e   d e   l a   c a r a   q u e   n o   h a b í a   d e   s e r   d e   l a   t a n   d e Seed: pudiera, a mi parecer, pintar y des\n",
            "Texto generado: pudiera, a mi parecer, pintar y des\n",
            "3125/3125 [==============================] - 171s 55ms/step - loss: 1.5359 - accuracy: 0.5224\n",
            "Epoch 7/15\n",
            "3125/3125 [==============================] - ETA: 0s - loss: 1.5250 - accuracy: 0.5265\n",
            "\n",
            "\n",
            "\n",
            "------> Epoch: 7 - Generando texto con temperature 0.2\n",
            "u e s a ,   a   l o   q u e   p o r   d e   c a b a l l e r o   d e   l a   c a r a c i a   d e   l a s   c a r t a s   d e   l a   m a n o   e n   l a   m a n o   e n   e l   c a r t o   p o r   e s t a   m e   h a l l a   d e   l a   m a n o   e l   c a b a l l e r o   d e   l a   m a n c h a   d e   l a   m a n o ,   y   e s t e   m i   p a r a   d e   l a   m e n a d o   d e   l a   m a n c h a   d e   l a   m a n c h a ,   p o r q u e   l o   c o m o   e s t a   m e   h a b í a   d e   s e r   d e   l a   m a n o ,   y   e s t a   m e   h a l l a   d e   l a   c a r a   d e   s u   m i s Seed: caminaron al castillo. mandó la duq\n",
            "Texto generado: caminaron al castillo. mandó la duq\n",
            "3125/3125 [==============================] - 171s 55ms/step - loss: 1.5250 - accuracy: 0.5265\n",
            "Epoch 8/15\n",
            "3124/3125 [============================>.] - ETA: 0s - loss: 1.5137 - accuracy: 0.5288\n",
            "\n",
            "\n",
            "\n",
            "------> Epoch: 8 - Generando texto con temperature 0.2\n",
            "d e   c o n t e n t o   d e   l a   c a r a   d e   l a   c a b a l l e r o   a n d a n t e   d e   l a   m a n o   a   l o   q u e   e s t a b a   b u e n o   q u e   s e   l e   a m i g o   d e   l a   m e n a d o   d e   l a   m a l a   a   l o   q u e   s e   p u e d e n   d e   l a   c a r a   d e   l a   c a b a l l e r o   d e   l a   m a l i e n t e   d e   l a   m a l a   d e   l a   c a r a c a   y   d e   l a   c a r a   a   l a   c a r a   q u e   l a   c a b a l l e r o   d e   l a   c a n t e   a   l a   c a b a l l e r   d e   l a   c a b a l l e r o   d e   l a   p e r s a d a Seed: se anda a decir verdades ese señor \n",
            "Texto generado: se anda a decir verdades ese señor \n",
            "3125/3125 [==============================] - 173s 55ms/step - loss: 1.5138 - accuracy: 0.5287\n",
            "Epoch 9/15\n",
            "3125/3125 [==============================] - ETA: 0s - loss: 1.5047 - accuracy: 0.5315\n",
            "\n",
            "\n",
            "\n",
            "------> Epoch: 9 - Generando texto con temperature 0.2\n",
            "e r   e n   l a   m e r a d a ,   d e   l a   s e ñ o r a   d e   d e   l a   p a l l a ,   y   a l   m i s m o   q u e   s e   l e   h a b í a   d e   s e r   d e   l a   c a b a l l e r o   a n d a n t e   y   a   l a   m e s c e d a   c o n   e l   c a b a l l e r o   d e   l a   m e n t e   d e   l a   m e r c e d   q u e   l e   h a b í a   d e   s e r   a l   c o n s i n t e r a ,   y   a l   m u n d o   y   d e   l a   c a r t e   a   l a   m e s t e   a   l a   m e n t e   d e   l a   m e r c e d   q u e   l a   c a b a l l e r o   a n d a n t e   s e   p e r s o n a   l a   a l l a   Seed: sancho había contado, de que no poc\n",
            "Texto generado: sancho había contado, de que no poc\n",
            "3125/3125 [==============================] - 172s 55ms/step - loss: 1.5047 - accuracy: 0.5315\n",
            "Epoch 10/15\n",
            "3124/3125 [============================>.] - ETA: 0s - loss: 1.4961 - accuracy: 0.5333\n",
            "\n",
            "\n",
            "\n",
            "------> Epoch: 10 - Generando texto con temperature 0.2\n",
            "  l a   c o n t a n t a ,   y   e l   c a b a l l e r o   d e   l a   m a n o   e n   l a   c a b a l l e r o   a n d a n t e   s e   l o   c o m o   d e s t a   v e r t a d   d e   l a   c a b a l l e r o s a   d e   l a   c a r a   q u e   l e   h a b í a   d e   h a b í a   d e   m e r m o s   d e   l a   c a b a l l e r o   a n d a n t e   c o n   e l   c u r a   y   d e   l a   c a b a l l e r e s   a n d a n t e s   d e   l a   m e r m a d a   d e   l a   m a n o ,   y   a l   s i s t o   d e   l a   c a r a c i a   d e   l a   c a r t e   d e   l a   m e s t a   m e   a c a b a r   a   Seed: nta madre iglesia?'', yo saqué toda\n",
            "Texto generado: nta madre iglesia?'', yo saqué toda\n",
            "3125/3125 [==============================] - 171s 55ms/step - loss: 1.4962 - accuracy: 0.5333\n",
            "Epoch 11/15\n",
            "3125/3125 [==============================] - ETA: 0s - loss: 1.4898 - accuracy: 0.5357\n",
            "\n",
            "\n",
            "\n",
            "------> Epoch: 11 - Generando texto con temperature 0.2\n",
            "d e   l a   m e r c e d   l a   m e n o s   d e   l a   m e n a s   d e   l a   m e r c e d   d e   l a   m e n a s   d e   l a   m e s t e   h a l l a   d e   l a   m e n o s   d e   l a   m a n t e ,   y   a l   d e   l a   m e n o s   d e   l a   m a n c h a ,   y   a   l o   q u e   e s t a b a   d e   m i   a m a   d e   l a   m e n o s   d e   l a   m e n t e   d e   a q u e l l a   a l t a   d e   l a   m e n t e   d e   m i   a m i g o   d e   l a   m e n a s   d e   l a   m a l i d a   d e   l a   m e n a s   d e   l a   m e n o s   d e   l a   m e n t e   d e   l a   m a n o   a   l Seed: n de caballería, y hoy ha\n",
            "desfecho \n",
            "Texto generado: n de caballería, y hoy ha\n",
            "desfecho \n",
            "3125/3125 [==============================] - 172s 55ms/step - loss: 1.4898 - accuracy: 0.5357\n",
            "Epoch 12/15\n",
            "3124/3125 [============================>.] - ETA: 0s - loss: 1.4817 - accuracy: 0.5377\n",
            "\n",
            "\n",
            "\n",
            "------> Epoch: 12 - Generando texto con temperature 0.2\n",
            "s   a l g u n a s   d e   l a   m a n o   e l   a l m a r   d e   l a   c a r t e   d e   l a   m a n o   e l   a l m a n d o   a   e l   p a c i e n t o   d e   l a   c a r t e   d e   l a   c a r a c i a n d o   e n   l a   c a r t e   d e   s u   a l t a   d e   l a   m e n t e ,   y   e l   c a b a l l e r o   d e   l a   c a r t e   d e   l a   p e r s a d o   a   s u   a m i g o   d e   l a   c a r t e   d e   l a   m e n t e ,   y   a l   m u n d o   y   p o r   l a   c a b a l l e r o   d e   l a   m a n o   e l   a l m a n d o   e n   l a   m e n o r   d e   l a   m e n o s   a   l a Seed: ensa dirigirlos.\n",
            "\n",
            "-señores y grande\n",
            "Texto generado: ensa dirigirlos.\n",
            "\n",
            "-señores y grande\n",
            "3125/3125 [==============================] - 172s 55ms/step - loss: 1.4817 - accuracy: 0.5377\n",
            "Epoch 13/15\n",
            "3125/3125 [==============================] - ETA: 0s - loss: 1.4758 - accuracy: 0.5392\n",
            "\n",
            "\n",
            "\n",
            "------> Epoch: 13 - Generando texto con temperature 0.2\n",
            "  a   l o   q u e   s e   s e   l a   c a b a l l e r o   a n d a n t e   d e   l a   c a r t e   d e   l a   m a n c h a   d e   l a   m a n c h a ,   q u e   e n   l a   m a n o   e n   e l   c a r a   q u e   e n   l a   s e ñ o r a   d o n   q u i j o t e   d e   l a   m e n t e   d e   l a   m a n c h a ,   p o r   l a   m e s c e d a   d e   l a   m e s t e   c o n t e n t a   y   d e   l a   c a b a l l e r o   a n d a n t e   d e   l o s   q u e   l e   h a b í a   d e   l a   m a n t e   h a b í a   e s t a r   p a r a   d e   l a   c o n t e n c i ó n   d e   l a   c o n t e n t e , Seed: sase que así,\n",
            "acaso y sin pensar, y\n",
            "Texto generado: sase que así,\n",
            "acaso y sin pensar, y\n",
            "3125/3125 [==============================] - 173s 55ms/step - loss: 1.4758 - accuracy: 0.5392\n",
            "Epoch 14/15\n",
            "3125/3125 [==============================] - ETA: 0s - loss: 1.4694 - accuracy: 0.5387\n",
            "\n",
            "\n",
            "\n",
            "------> Epoch: 14 - Generando texto con temperature 0.2\n",
            "  e s   l a   m a n o   q u e   s e   l a s   t a l   c a b a l l e r o s   d e   l a   m a n o   q u e   l a   d e l   c u e l p o   d e   l a   c a b a l l e r o   d e   l a   m a n o   e l   c a b a l l e r o   d e   l a   m a n o   e n   l a   c a b a l l e r o s   a l g u n o s   c o m o s e s   s e   l e   h a b í a   d e   l a   m e n t e   d e   l a   c a r a   q u e   s e   h a b í a   d e   l a   c a b a l l e r o   a n d a n t e   s e   h a b í a   d e   s e r   e l   c u a l   d e   l a   c a n t a   d e   l a   c a b a l l e r o   l a s   d e   l a   c a b a l l e r í a   d e   l Seed: nos ha conducido!, porque, si yo no\n",
            "Texto generado: nos ha conducido!, porque, si yo no\n",
            "3125/3125 [==============================] - 171s 55ms/step - loss: 1.4694 - accuracy: 0.5387\n",
            "Epoch 15/15\n",
            "3124/3125 [============================>.] - ETA: 0s - loss: 1.4598 - accuracy: 0.5444\n",
            "\n",
            "\n",
            "\n",
            "------> Epoch: 15 - Generando texto con temperature 0.2\n",
            "  l o   q u e   l a   c a b a l l e r o   a n d a n t e   d e   l a   m a n c h a ,   y   n o   l e   h a b í a   d e   d e   l a   m e s c i d e d a d   d e   l a   c a r t e   d e   l a   c a b a l l e r o   d e   l a   c a r t e   d e   l a   c a r t e   d e   l a   c a r t e   d e   l a   c a r t e   d e   l a   m e n t e   d e   s u   c a b a l l e r o   a n d a n t e   s a n c h o   d e   l a   m e s t r a   d e   l a   c a r t a   d e   l a   d e l   m a n d o   y   d e   l a   c a r t e   q u e   e s t a b a   e s t a b a   p a r a   d e   l a   m e n t e   d e   l a   c a r t e   d e Seed: s mayores, que quiero tomar\n",
            "de todo\n",
            "Texto generado: s mayores, que quiero tomar\n",
            "de todo\n",
            "3125/3125 [==============================] - 171s 55ms/step - loss: 1.4597 - accuracy: 0.5444\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "<keras.src.callbacks.History at 0x7a068b003310>"
            ]
          },
          "execution_count": 18,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "\n",
        "\n",
        "model.fit(X, y, batch_size=32, epochs=15, callbacks=generation_callback)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "PJp9Ds55s2O-"
      },
      "outputs": [],
      "source": [
        "TEMPERATURES_TO_TRY = [0.5] #, 0.5, 1.0, 1.2]\n",
        "GENERATED_TEXT_LENGTH = 300\n",
        "\n",
        "def generate_text(seed_text, model, length=300, temperature=1, max_length=30):\n",
        "    \"\"\"Genera una secuencia de texto a partir de seed_text utilizando model.\n",
        "\n",
        "    La secuencia tiene longitud length y el sampling se hace con la temperature\n",
        "    definida.\n",
        "    \"\"\"\n",
        "\n",
        "    # Aquí guardaremos nuestro texto generado, que incluirá el\n",
        "    # texto origen\n",
        "    generated = seed_text\n",
        "\n",
        "    # Utilizar el modelo en un bucle de manera que generemos\n",
        "    # carácter a carácter. Habrá que construir los valores de\n",
        "    # X_pred de manera similar a como hemos hecho arriba, salvo que\n",
        "    # aquí sólo se necesita una oración\n",
        "    # Nótese que el x que utilicemos tiene que irse actualizando con\n",
        "    # los caracteres que se van generando. La secuencia de entrada al\n",
        "    # modelo tiene que ser una secuencia de tamaño SEQ_LENGTH que\n",
        "    # incluya el último caracter predicho.\n",
        "\n",
        "    prediction = []\n",
        "\n",
        "    for i in range(length):\n",
        "        # Make numpy array to hold seed\n",
        "        X = np.zeros((1, len(generated), len(chars) ))\n",
        "\n",
        "        # Set one-hot vectors for seed sequence\n",
        "        for t, char in enumerate(seed_text):\n",
        "            X[0, t, char_indices[char]] = 1\n",
        "\n",
        "        # Generate prediction for next character\n",
        "        preds = model.predict(X, verbose=0)[0]\n",
        "        # Choose a character from the prediction probabilities\n",
        "        next_index = sample(preds,0.2)\n",
        "        next_char = indice_char[next_index]\n",
        "\n",
        "        prediction.append(next_char)\n",
        "        # Add the predicted character to the seed sequence so the next prediction\n",
        "        # includes this character in it's seed.\n",
        "        #generated += next_char\n",
        "        seed_text = seed_text[1:] + next_char\n",
        "\n",
        "        print(next_char, end= \" \");\n",
        "        # Flush so we can see the prediction as it's generated\n",
        "        sys.stdout.flush()\n",
        "\n",
        "    prediction = ''.join(prediction)\n",
        "    sys.stdout.flush()\n",
        "\n",
        "    return generated\n",
        "\n",
        "\n",
        "def on_epoch_end(epoch, logs):\n",
        "  print(\"\\n\\n\\n\")\n",
        "\n",
        "  # Primero, seleccionamos una secuencia al azar para empezar a predecir\n",
        "  # a partir de ella\n",
        "  start_pos = random.randint(0, len(text) - SEQ_LENGTH - 1)\n",
        "  seed_text = text[start_pos:start_pos + SEQ_LENGTH]\n",
        "  for temperature in TEMPERATURES_TO_TRY:\n",
        "    print(\"------> Epoch: {} - Generando texto con temperature {}\".format(\n",
        "        epoch + 1, temperature))\n",
        "\n",
        "    generated_text = generate_text(seed_text, model,\n",
        "                                   GENERATED_TEXT_LENGTH, temperature)\n",
        "    print(\"Seed: {}\".format(seed_text))\n",
        "    print(\"Texto generado: {}\".format(generated_text))\n",
        "\n",
        "\n",
        "generation_callback = LambdaCallback(on_epoch_end=on_epoch_end)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GneURbWZxN8d",
        "outputId": "53d4b150-8292-42ed-8f91-9b1725225769"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "3124/3125 [============================>.] - ETA: 0s - loss: 1.4526 - accuracy: 0.5445\n",
            "\n",
            "\n",
            "\n",
            "------> Epoch: 1 - Generando texto con temperature 0.5\n",
            "a   s u   p a r t e   d e   l a   m a n o   e l   c a b a l l e r o   d e   l a   m a n o   e n   e l   c u a l   c o m o   d e   l a   c o n t e n c i ó n   d e   s u   a m o   d e   l a   p a l l a   d e   l a   c a r t e ,   y   a   l o   q u e   l a s   c a b a l l e r o s   d e   l a   m e n t e   d e   l a   c a r a c i a   d e   l a   m a n o   e l   c a b a l l e r o   d e   l a   c a r a   y   d e   l a   c a r t e ,   y   s e   e s p e r a   d e   l a   c a r t e   a   l a   m e s t e   c a r a   d e   l a   c a r a c i a   d e   l a   c a b a l l e r í a   d e   s u   p a r t e ,   Seed:  paso que había\n",
            "entrado, se volvió \n",
            "Texto generado:  paso que había\n",
            "entrado, se volvió \n",
            "3125/3125 [==============================] - 174s 56ms/step - loss: 1.4525 - accuracy: 0.5445\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "<keras.src.callbacks.History at 0x7a071a3e3340>"
            ]
          },
          "execution_count": 20,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "model.fit(X, y, batch_size=32, epochs=1, callbacks=generation_callback)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "-G92Lvac3G3S"
      },
      "outputs": [],
      "source": [
        "TEMPERATURES_TO_TRY = [1.0] #, 0.5, 1.0, 1.2]\n",
        "GENERATED_TEXT_LENGTH = 300\n",
        "\n",
        "def generate_text(seed_text, model, length=300, temperature=1, max_length=30):\n",
        "    \"\"\"Genera una secuencia de texto a partir de seed_text utilizando model.\n",
        "\n",
        "    La secuencia tiene longitud length y el sampling se hace con la temperature\n",
        "    definida.\n",
        "    \"\"\"\n",
        "\n",
        "    # Aquí guardaremos nuestro texto generado, que incluirá el\n",
        "    # texto origen\n",
        "    generated = seed_text\n",
        "\n",
        "    # Utilizar el modelo en un bucle de manera que generemos\n",
        "    # carácter a carácter. Habrá que construir los valores de\n",
        "    # X_pred de manera similar a como hemos hecho arriba, salvo que\n",
        "    # aquí sólo se necesita una oración\n",
        "    # Nótese que el x que utilicemos tiene que irse actualizando con\n",
        "    # los caracteres que se van generando. La secuencia de entrada al\n",
        "    # modelo tiene que ser una secuencia de tamaño SEQ_LENGTH que\n",
        "    # incluya el último caracter predicho.\n",
        "\n",
        "    prediction = []\n",
        "\n",
        "    for i in range(length):\n",
        "        # Make numpy array to hold seed\n",
        "        X = np.zeros((1, len(generated), len(chars) ))\n",
        "\n",
        "        # Set one-hot vectors for seed sequence\n",
        "        for t, char in enumerate(seed_text):\n",
        "            X[0, t, char_indices[char]] = 1\n",
        "\n",
        "        # Generate prediction for next character\n",
        "        preds = model.predict(X, verbose=0)[0]\n",
        "        # Choose a character from the prediction probabilities\n",
        "        next_index = sample(preds,0.2)\n",
        "        next_char = indice_char[next_index]\n",
        "\n",
        "        prediction.append(next_char)\n",
        "        # Add the predicted character to the seed sequence so the next prediction\n",
        "        # includes this character in it's seed.\n",
        "        #generated += next_char\n",
        "        seed_text = seed_text[1:] + next_char\n",
        "\n",
        "        print(next_char, end= \" \");\n",
        "        # Flush so we can see the prediction as it's generated\n",
        "        sys.stdout.flush()\n",
        "\n",
        "    prediction = ''.join(prediction)\n",
        "    sys.stdout.flush()\n",
        "\n",
        "\n",
        "    return generated\n",
        "\n",
        "\n",
        "def on_epoch_end(epoch, logs):\n",
        "  print(\"\\n\\n\\n\")\n",
        "\n",
        "  # Primero, seleccionamos una secuencia al azar para empezar a predecir\n",
        "  # a partir de ella\n",
        "  start_pos = random.randint(0, len(text) - SEQ_LENGTH - 1)\n",
        "  seed_text = text[start_pos:start_pos + SEQ_LENGTH]\n",
        "  for temperature in TEMPERATURES_TO_TRY:\n",
        "    print(\"------> Epoch: {} - Generando texto con temperature {}\".format(\n",
        "        epoch + 1, temperature))\n",
        "\n",
        "    generated_text = generate_text(seed_text, model,\n",
        "                                   GENERATED_TEXT_LENGTH, temperature)\n",
        "    print(\"Seed: {}\".format(seed_text))\n",
        "    print(\"Texto generado: {}\".format(generated_text))\n",
        "\n",
        "\n",
        "generation_callback = LambdaCallback(on_epoch_end=on_epoch_end)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MKPhrb4i3QJp",
        "outputId": "f65bf063-5901-43b6-dad3-80e2c5c798c0"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "3125/3125 [==============================] - ETA: 0s - loss: 1.4514 - accuracy: 0.5462\n",
            "\n",
            "\n",
            "\n",
            "------> Epoch: 1 - Generando texto con temperature 1.0\n",
            "s ,   y   a   s u   s e ñ o r a   d e   l a   v e r d a d ,   y   a   l o   q u e   l a   s e ñ o r a   m u e s t r a   m e r c e d   d e   l a   m e n t e   c o n   e l l o s   s e   l a   c a b a l l e r o   a   l a s   c a r a l l a r   a   l a   m a n o   e l   c a b a l l e r o   d e   l a   v e r d a d ,   y   e s t a   d e   l a   c a r t e   d e   l a   m e r a d o   d e   l a   m e n t e   c o n   l a   c a r a   q u e   s e   m e   a c a b a r a   e s t a   m a n o   a   l a   m e s t e   e n   l a   m e n t e   d e   l a   m e r a d o   d e   l a   c a r t e   a   l a   m e r a d o Seed:  se le acabasen de salir los diente\n",
            "Texto generado:  se le acabasen de salir los diente\n",
            "3125/3125 [==============================] - 171s 55ms/step - loss: 1.4514 - accuracy: 0.5462\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "<keras.src.callbacks.History at 0x7a0689f7f160>"
            ]
          },
          "execution_count": 22,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "model.fit(X, y, batch_size=32, epochs=1, callbacks=generation_callback)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "SZb-_f9J3Phz"
      },
      "outputs": [],
      "source": [
        "TEMPERATURES_TO_TRY = [1.2] #, 0.5, 1.0, 1.2]\n",
        "GENERATED_TEXT_LENGTH = 300\n",
        "\n",
        "def generate_text(seed_text, model, length=300, temperature=1, max_length=30):\n",
        "    \"\"\"Genera una secuencia de texto a partir de seed_text utilizando model.\n",
        "\n",
        "    La secuencia tiene longitud length y el sampling se hace con la temperature\n",
        "    definida.\n",
        "    \"\"\"\n",
        "\n",
        "    # Aquí guardaremos nuestro texto generado, que incluirá el\n",
        "    # texto origen\n",
        "    generated = seed_text\n",
        "\n",
        "    # Utilizar el modelo en un bucle de manera que generemos\n",
        "    # carácter a carácter. Habrá que construir los valores de\n",
        "    # X_pred de manera similar a como hemos hecho arriba, salvo que\n",
        "    # aquí sólo se necesita una oración\n",
        "    # Nótese que el x que utilicemos tiene que irse actualizando con\n",
        "    # los caracteres que se van generando. La secuencia de entrada al\n",
        "    # modelo tiene que ser una secuencia de tamaño SEQ_LENGTH que\n",
        "    # incluya el último caracter predicho.\n",
        "\n",
        "    prediction = []\n",
        "\n",
        "    for i in range(length):\n",
        "        # Make numpy array to hold seed\n",
        "        X = np.zeros((1, len(generated), len(chars) ))\n",
        "\n",
        "        # Set one-hot vectors for seed sequence\n",
        "        for t, char in enumerate(seed_text):\n",
        "            X[0, t, char_indices[char]] = 1\n",
        "\n",
        "        # Generate prediction for next character\n",
        "        preds = model.predict(X, verbose=0)[0]\n",
        "        # Choose a character from the prediction probabilities\n",
        "        next_index = sample(preds,0.2)\n",
        "        next_char = indice_char[next_index]\n",
        "\n",
        "        prediction.append(next_char)\n",
        "        # Add the predicted character to the seed sequence so the next prediction\n",
        "        # includes this character in it's seed.\n",
        "        #generated += next_char\n",
        "        seed_text = seed_text[1:] + next_char\n",
        "\n",
        "        print(next_char, end= \" \");\n",
        "        # Flush so we can see the prediction as it's generated\n",
        "        sys.stdout.flush()\n",
        "\n",
        "    prediction = ''.join(prediction)\n",
        "    sys.stdout.flush()\n",
        "    \n",
        "    return generated\n",
        "\n",
        "\n",
        "def on_epoch_end(epoch, logs):\n",
        "  print(\"\\n\\n\\n\")\n",
        "\n",
        "  # Primero, seleccionamos una secuencia al azar para empezar a predecir\n",
        "  # a partir de ella\n",
        "  start_pos = random.randint(0, len(text) - SEQ_LENGTH - 1)\n",
        "  seed_text = text[start_pos:start_pos + SEQ_LENGTH]\n",
        "  for temperature in TEMPERATURES_TO_TRY:\n",
        "    print(\"------> Epoch: {} - Generando texto con temperature {}\".format(\n",
        "        epoch + 1, temperature))\n",
        "\n",
        "    generated_text = generate_text(seed_text, model,\n",
        "                                   GENERATED_TEXT_LENGTH, temperature)\n",
        "    print(\"Seed: {}\".format(seed_text))\n",
        "    print(\"Texto generado: {}\".format(generated_text))\n",
        "\n",
        "\n",
        "generation_callback = LambdaCallback(on_epoch_end=on_epoch_end)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "u53nDV9j3Yje",
        "outputId": "974e33b0-18f1-4e52-9cad-384b3276f6c2"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "3124/3125 [============================>.] - ETA: 0s - loss: 1.4420 - accuracy: 0.5495\n",
            "\n",
            "\n",
            "\n",
            "------> Epoch: 1 - Generando texto con temperature 1.2\n",
            "s   d e   l a   c a r t e   d e   l a   m a n e r a   a   s u   a m i g o   a   l a   m e n t e   d e   l a   m e n t e ,   y   a l   s u s   d e   l a   s e ñ o r a   d e   c u r a   q u e   s e   l e   h a b í a   d e   h a b e r   q u e   e s t a b a   l o s   d e   l a   m a n o   e n   e l   c u e l a   d e   l a   c a r t e   d e   l a   c a r t e   d e   l a   m i s m a   d e   l a   p r e s t e   a   l a   m e r a d o   d e   t o d a   l a   m e s t r a c i ó   d e   l a   c a b a l l e r o   a n d a n t e   s e   l e   h a b í a   d e   s e r   s u   h a b í a   d e   l a   c a r t e Seed:  toda la sala levantando los dosele\n",
            "Texto generado:  toda la sala levantando los dosele\n",
            "3125/3125 [==============================] - 177s 57ms/step - loss: 1.4420 - accuracy: 0.5495\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "<keras.src.callbacks.History at 0x7a0689eba650>"
            ]
          },
          "execution_count": 24,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "model.fit(X, y, batch_size=32, epochs=1, callbacks=generation_callback)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.9"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
